---
title: 'Introduction to Economic Growth Series-The Solow Model'
date: 2023-03-07
permalink: /posts/2020/blog-post-8/
tags:
  - AI
categories:
  - Machine Learning
  - Study
---

Transformers were developed to overcome two major limitations of RNN. 
1. Slow training procedure
2. Difficult to process longer sequences

## Attention mechanism
Unlike previous models, transformers gets rid of labels and utilize attention mechanism to concentrate on words that have close relationship with other words. <br>

Think of the following sentences: 
|I moved the chairs from the room to the storage until it was empty| it = room|
|:---: | :---: |
|I moved the chairs from the room to the storage until it was full| it = garage| 

Attention mechanisms allow machine to find the relationship of words as human naturally do.


## Transformer Architecture
Transformers architecture is divided into ***Encoder stack*** and ***Decoder stack***. Encoder stacks (left side of the image below) learns the expressions and the Decoder creates a sentence based on the expressions learned during the Encoding Process.


!["Attention Is All You Need" (Vaswani et al., 2017)](https://github.com/elias-lee/lelias.github.io/blob/master/_posts/resources/ml_study/transformer_architecture.png?raw=true)

### Encoder

!["Attention Is All You Need" (Vaswani et al., 2017)](https://github.com/elias-lee/lelias.github.io/blob/master/_posts/resources/ml_study/transformer_encoder.png?raw=true)

#### sub-layers
The encoder consist of 6 identical layers that are composed of two sub-layers. 

1. First sublayer is composed of Multi-Head attention mechanism. 
2. Second Sublayer feed-forward network with ReLU activation.
   * $FFN(x) = ReLU (W_1x + b_1)W_2 + b_2$

Although each layer implements the same linear transformations weight ($W$) and bias ($b$) are different.

#### normalizing layer
In each layer there is also a normalizing layer that normalizes the sum between input (x) and the output generated by the sub-layer:
$$\text{norm}(x + \text{sub}(x))$$

#### Positional Encoding
Transformers disregards the position of the words. Thus to not lose positional information an additional positional encoding must be added. 
Positional encoding vectors have the same dimension as the input embeddings. Generated positional encoding vectors are added to the input embedding. 

### Decoder





